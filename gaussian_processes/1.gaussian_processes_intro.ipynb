{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Notes**\n",
    "- GP generalizes multivariate normal to _infinite_ dimension\n",
    "- GP = _**distribution over functions**_\n",
    "- GP is fully specified by a _**mean**_ function and _**covariance**_ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mApplications\u001b[m\u001b[m      \u001b[1m\u001b[36mLibrary\u001b[m\u001b[m           Untitled.ipynb    sdr-fmems.sdm\r\n",
      "\u001b[1m\u001b[36mDesktop\u001b[m\u001b[m           \u001b[1m\u001b[36mMovies\u001b[m\u001b[m            \u001b[1m\u001b[36mfonts\u001b[m\u001b[m             \u001b[1m\u001b[36mworkspace\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mDocuments\u001b[m\u001b[m         \u001b[1m\u001b[36mMusic\u001b[m\u001b[m             \u001b[1m\u001b[36mnltk_data\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mDownloads\u001b[m\u001b[m         \u001b[1m\u001b[36mPictures\u001b[m\u001b[m          \u001b[1m\u001b[36mrepos\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mEnvs\u001b[m\u001b[m              \u001b[1m\u001b[36mPublic\u001b[m\u001b[m            \u001b[1m\u001b[36mscikit_learn_data\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_layers, num_labels, pre_trained_weights, dropout, bi_dir):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bi_dir = 2 if bi_dir else 1\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight.data=torch.Tensor(pre_trained_weights)\n",
    "        #self.rnn = nn.GRU(input_size = embedding_dim,\n",
    "        ##                    hidden_size = hidden_dim,\n",
    "        #                    num_layers = num_layers,\n",
    "        #                    bidirectional = bi_dir\n",
    "        #                   )\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*self.bi_dir, num_labels)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self, ):\n",
    "        # the first is the hidden h\n",
    "        return (Variable(torch.zeros(self.num_layers*self.bi_dir, 1,self.hidden_dim)))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        x = self.word_embeddings(sentence)\n",
    "        x = self.drop(x)\n",
    "        x = x.view(len(sentence), 1, -1)\n",
    "        for i in range(self.num_layers):\n",
    "            gru_out, self.hidden = self.rnn(x, self.hidden)\n",
    "        out = self.hidden2label(gru_out[-1])\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n",
    "    \n",
    "    def make_context_vector(self, seq, to_ix):\n",
    "        idxs = [to_ix[w] for w in seq.split()]\n",
    "        tensor = torch.LongTensor(idxs)\n",
    "        return tensor\n",
    "    \n",
    "    def make_target(self, label, label_to_idx):\n",
    "        return torch.LongTensor([label_to_idx[label]])\n",
    "    \n",
    "    def get_reply_proba(self, msg):\n",
    "        x = transformText(msg,do_stop=False,do_stem=False)\n",
    "        x = Variable(self.make_context_vector(x,word_to_ix))\n",
    "        log_probs = self.forward(x)   \n",
    "        _,predicted = torch.max(log_probs.data,1)\n",
    "        pred_label=list(label_to_ix.keys())[list(label_to_ix.values()).index(predicted[0])]\n",
    "        ## Getting probabilities\n",
    "        probs = F.softmax(Variable(log_probs.data),dim=1)\n",
    "        probs = probs.data[0].numpy()\n",
    "        probs_frame = pd.DataFrame({'probs':probs,'label':list(label_to_ix)})\n",
    "        probs_frame=probs_frame.sort_values(['probs'], ascending=False)[0:3].reset_index(drop=True)\n",
    "        return pred_label,probs_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
